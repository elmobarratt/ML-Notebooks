{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ffd436-5aa8-4d07-b525-bfc4c86f1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleGCN:\n",
    "    def __init__(self, in_features, hidden_features, out_features, lr=0.01, seed=0):\n",
    "        np.random.seed(seed)\n",
    "        # Xavier initialization\n",
    "        limit1 = np.sqrt(6 / (in_features + hidden_features))\n",
    "        limit2 = np.sqrt(6 / (hidden_features + out_features))\n",
    "        \n",
    "        self.W1 = np.random.uniform(-limit1, limit1, (in_features, hidden_features))\n",
    "        self.W2 = np.random.uniform(-limit2, limit2, (hidden_features, out_features))\n",
    "        self.lr = lr\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_grad(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, A_norm, X):\n",
    "        \"\"\" Forward pass with cached intermediates for backprop \"\"\"\n",
    "        self.X = X\n",
    "        self.A_norm = A_norm\n",
    "\n",
    "        self.H_pre = A_norm @ X @ self.W1     # before ReLU\n",
    "        self.H = self.relu(self.H_pre)        # hidden layer\n",
    "        self.Z = A_norm @ self.H @ self.W2    # logits\n",
    "        self.out = self.softmax(self.Z)       # probabilities\n",
    "        return self.out\n",
    "\n",
    "    def compute_loss(self, Y_true):\n",
    "        \"\"\" Cross-entropy loss \"\"\"\n",
    "        m = Y_true.shape[0]\n",
    "        log_likelihood = -np.log(self.out[range(m), Y_true] + 1e-9)\n",
    "        return np.sum(log_likelihood) / m\n",
    "\n",
    "    def backward(self, Y_true):\n",
    "        \"\"\" Backpropagation for W1 and W2 \"\"\"\n",
    "        m = Y_true.shape[0]\n",
    "        Y_onehot = np.zeros_like(self.out)\n",
    "        Y_onehot[np.arange(m), Y_true] = 1\n",
    "\n",
    "        # dL/dZ\n",
    "        dZ = (self.out - Y_onehot) / m\n",
    "\n",
    "        # Gradients for W2\n",
    "        dW2 = self.H.T @ (self.A_norm.T @ dZ)\n",
    "\n",
    "        # Backprop to hidden\n",
    "        dH = (self.A_norm @ dZ) @ self.W2.T\n",
    "        dH_pre = dH * self.relu_grad(self.H_pre)\n",
    "\n",
    "        # Gradients for W1\n",
    "        dW1 = self.X.T @ (self.A_norm.T @ dH_pre)\n",
    "\n",
    "        # Update weights\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.W2 -= self.lr * dW2\n",
    "\n",
    "    def train(self, A, X, Y, epochs=200):\n",
    "        # Precompute normalized adjacency\n",
    "        I = np.eye(A.shape[0])\n",
    "        A_hat = A + I\n",
    "        D_hat = np.diag(1.0 / np.sqrt(np.sum(A_hat, axis=1)))\n",
    "        A_norm = D_hat @ A_hat @ D_hat\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            out = self.forward(A_norm, X)\n",
    "            loss = self.compute_loss(Y)\n",
    "            self.backward(Y)\n",
    "            if epoch % 200 == 0 or epoch == epochs-1:\n",
    "                preds = np.argmax(out, axis=1)\n",
    "                acc = np.mean(preds == Y)\n",
    "                print(f\"Epoch {epoch:03d}: loss={loss:.4f}, acc={acc:.4f}\")\n",
    "\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e218e07e-cc48-4b1c-b691-9cebd4274aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: loss=0.7209, acc=0.5000\n",
      "Epoch 200: loss=0.6689, acc=0.7500\n",
      "Epoch 400: loss=0.6621, acc=0.7500\n",
      "Epoch 600: loss=0.6564, acc=0.7500\n",
      "Epoch 800: loss=0.6518, acc=0.7500\n",
      "Epoch 1000: loss=0.6471, acc=0.7500\n",
      "Epoch 1200: loss=0.6409, acc=0.7500\n",
      "Epoch 1400: loss=0.6336, acc=0.7500\n",
      "Epoch 1600: loss=0.6266, acc=0.7500\n",
      "Epoch 1800: loss=0.6184, acc=0.7500\n",
      "Epoch 2000: loss=0.6077, acc=0.7500\n",
      "Epoch 2200: loss=0.5945, acc=0.7500\n",
      "Epoch 2400: loss=0.5793, acc=0.7500\n",
      "Epoch 2600: loss=0.5632, acc=0.7500\n",
      "Epoch 2800: loss=0.5475, acc=0.7500\n",
      "Epoch 3000: loss=0.5335, acc=0.7500\n",
      "Epoch 3200: loss=0.5219, acc=0.7500\n",
      "Epoch 3400: loss=0.5126, acc=0.7500\n",
      "Epoch 3600: loss=0.5062, acc=0.7500\n",
      "Epoch 3800: loss=0.5022, acc=0.7500\n",
      "Epoch 4000: loss=0.4993, acc=0.7500\n",
      "Epoch 4200: loss=0.4971, acc=0.7500\n",
      "Epoch 4400: loss=0.4954, acc=0.7500\n",
      "Epoch 4600: loss=0.4940, acc=0.7500\n",
      "Epoch 4800: loss=0.4930, acc=0.7500\n",
      "Epoch 5000: loss=0.4921, acc=0.7500\n",
      "Epoch 5200: loss=0.4914, acc=0.7500\n",
      "Epoch 5400: loss=0.4908, acc=0.7500\n",
      "Epoch 5600: loss=0.4903, acc=0.7500\n",
      "Epoch 5800: loss=0.4899, acc=0.7500\n",
      "Epoch 6000: loss=0.4895, acc=0.7500\n",
      "Epoch 6200: loss=0.4892, acc=0.7500\n",
      "Epoch 6400: loss=0.4889, acc=0.7500\n",
      "Epoch 6600: loss=0.4886, acc=0.7500\n",
      "Epoch 6800: loss=0.4884, acc=0.7500\n",
      "Epoch 7000: loss=0.4882, acc=0.7500\n",
      "Epoch 7200: loss=0.4881, acc=0.7500\n",
      "Epoch 7400: loss=0.4879, acc=0.7500\n",
      "Epoch 7600: loss=0.4877, acc=0.7500\n",
      "Epoch 7800: loss=0.4876, acc=0.7500\n",
      "Epoch 8000: loss=0.4875, acc=0.7500\n",
      "Epoch 8200: loss=0.4874, acc=0.7500\n",
      "Epoch 8400: loss=0.4873, acc=0.7500\n",
      "Epoch 8600: loss=0.4872, acc=0.7500\n",
      "Epoch 8800: loss=0.4871, acc=0.7500\n",
      "Epoch 9000: loss=0.4870, acc=0.7500\n",
      "Epoch 9200: loss=0.4870, acc=0.7500\n",
      "Epoch 9400: loss=0.4869, acc=0.7500\n",
      "Epoch 9600: loss=0.4868, acc=0.7500\n",
      "Epoch 9800: loss=0.4868, acc=0.7500\n",
      "Epoch 9999: loss=0.4867, acc=0.7500\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example graph (4 nodes)\n",
    "    A = np.array([\n",
    "        [0, 1, 0, 0],\n",
    "        [1, 0, 1, 1],\n",
    "        [0, 1, 0, 1],\n",
    "        [0, 1, 1, 0]\n",
    "    ], dtype=float)\n",
    "\n",
    "    # Node features: 4 nodes Ã— 3 features\n",
    "    X = np.random.randn(4, 3)\n",
    "\n",
    "    # Node labels (classes 0 or 1)\n",
    "    Y = np.array([0, 1, 0, 1])\n",
    "\n",
    "    gcn = SimpleGCN(in_features=3, hidden_features=5, out_features=2, lr=0.1)\n",
    "    gcn.train(A, X, Y, epochs=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53dd35fe-fb6b-4efd-b1d6-6f842c59ebd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.35355339, 0.        , 0.        ],\n",
       "       [0.35355339, 0.25      , 0.28867513, 0.28867513],\n",
       "       [0.        , 0.28867513, 0.33333333, 0.33333333],\n",
       "       [0.        , 0.28867513, 0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn.A_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c91bc2-1aaa-42a0-968e-eb41af538b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
