{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f748ae-dc95-4aef-b51f-b34554ec1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------------------\n",
    "# Helper functions\n",
    "# ---------------------\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1abe20d4-389a-4b61-8324-3f32eae38483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator: z -> x_fake\n",
    "class Generator:\n",
    "    def __init__(self):\n",
    "        # single linear layer: x_fake = w*z + b\n",
    "        self.w = np.random.randn() * 0.1\n",
    "        self.b = 0.0\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.w * z + self.b\n",
    "    \n",
    "    def params(self):\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "    def update(self, dw, db, lr=0.01):\n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb68a53-eef6-4c98-9eee-66e4806f5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator: x -> probability real\n",
    "class Discriminator:\n",
    "    def __init__(self):\n",
    "        # single-layer logistic regression\n",
    "        self.w = np.random.randn() * 0.1\n",
    "        self.b = 0.0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return sigmoid(self.w * x + self.b)\n",
    "    \n",
    "    def params(self):\n",
    "        return [self.w, self.b]\n",
    "    \n",
    "    def update(self, dw, db, lr=0.01):\n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46397ca2-86c9-4e42-bf4f-a2b0baf81122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss_D=1.406, Loss_G=0.693, Sample_fake_mean=-0.00\n",
      "Step 2000: Loss_D=1.371, Loss_G=0.884, Sample_fake_mean=4.25\n",
      "Step 4000: Loss_D=1.394, Loss_G=0.604, Sample_fake_mean=2.43\n",
      "Step 6000: Loss_D=1.389, Loss_G=0.741, Sample_fake_mean=3.28\n",
      "Step 8000: Loss_D=1.386, Loss_G=0.680, Sample_fake_mean=2.87\n",
      "Step 10000: Loss_D=1.387, Loss_G=0.710, Sample_fake_mean=3.06\n",
      "Step 12000: Loss_D=1.387, Loss_G=0.681, Sample_fake_mean=2.98\n",
      "Step 14000: Loss_D=1.386, Loss_G=0.695, Sample_fake_mean=3.01\n",
      "Step 16000: Loss_D=1.386, Loss_G=0.697, Sample_fake_mean=3.00\n",
      "Step 18000: Loss_D=1.386, Loss_G=0.700, Sample_fake_mean=3.00\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Training loop\n",
    "# ---------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "lr = 0.01\n",
    "num_steps = 20000\n",
    "batch_size = 32\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # 1. Sample real data from N(3,1)\n",
    "    real_data = np.random.normal(3, 1, batch_size)\n",
    "    \n",
    "    # 2. Sample noise and generate fake data\n",
    "    z = np.random.uniform(-1, 1, batch_size)\n",
    "    fake_data = G.forward(z)\n",
    "    \n",
    "    # 3. Discriminator forward\n",
    "    D_real = D.forward(real_data)\n",
    "    D_fake = D.forward(fake_data)\n",
    "    \n",
    "    # 4. Compute losses\n",
    "    loss_D = -np.mean(np.log(D_real + 1e-8) + np.log(1 - D_fake + 1e-8))\n",
    "    loss_G = -np.mean(np.log(D_fake + 1e-8))\n",
    "    \n",
    "    # 5. Backprop for Discriminator\n",
    "    dL_dDreal = -1 / (D_real + 1e-8)\n",
    "    dL_dDfake = -1 / (1 - D_fake + 1e-8)\n",
    "    \n",
    "    dDreal_dout = D_real * (1 - D_real)\n",
    "    dDfake_dout = D_fake * (1 - D_fake)\n",
    "    \n",
    "    grad_real = dL_dDreal * dDreal_dout\n",
    "    grad_fake = dL_dDfake * (-dDfake_dout)\n",
    "    \n",
    "    dDw = np.mean(grad_real * real_data + grad_fake * fake_data)\n",
    "    dDb = np.mean(grad_real + grad_fake)\n",
    "    \n",
    "    D.update(dDw, dDb, lr)\n",
    "    \n",
    "    # 6. Backprop for Generator (through D)\n",
    "    dL_dDfake_G = -1 / (D_fake + 1e-8)  # gradient of G’s loss wrt D output\n",
    "    grad_fake_G = dL_dDfake_G * dDfake_dout\n",
    "    \n",
    "    dG_fake = grad_fake_G * D.w   # propagate through D’s linear input\n",
    "    dGw = np.mean(dG_fake * z)\n",
    "    dGb = np.mean(dG_fake)\n",
    "    \n",
    "    G.update(dGw, dGb, lr)\n",
    "    \n",
    "    # 7. Logging\n",
    "    if step % 2000 == 0:\n",
    "        print(f\"Step {step}: Loss_D={loss_D:.3f}, Loss_G={loss_G:.3f}, \"\n",
    "              f\"Sample_fake_mean={np.mean(fake_data):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328463c9-0cd3-44aa-95c1-c4d1d80eaec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
