 {
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2df319-48b8-4a03-868e-8bc600fcc67b",
   "metadata": {},
   "source": [
    "# Multi-Layer Recurrent Neural Network (RNN)\n",
    "\n",
    "This implementation defines a **multi-layer RNN**.  \n",
    "The network consists of:\n",
    "\n",
    "- An **input layer** of size $d_\\text{in}$.\n",
    "- A sequence of **hidden recurrent layers** with dimensions specified by a list  \n",
    "  $$\n",
    "  [h_1, h_2, \\dots, h_L]\n",
    "  $$\n",
    "  where each $h_\\ell$ is the number of hidden units in layer $\\ell$.\n",
    "- An **output layer** of size $d_\\text{out}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward Dynamics\n",
    "\n",
    "At each time step $t$, the network receives an input vector  \n",
    "$$\n",
    "x_t \\in \\mathbb{R}^{d_\\text{in} \\times 1}.\n",
    "$$\n",
    "\n",
    "Each hidden layer $\\ell$ maintains its own hidden state $h_t^{(\\ell)}$.  \n",
    "The update rule for layer $\\ell$ is:\n",
    "\n",
    "$$\n",
    "h_t^{(\\ell)} = \\tanh\\!\\left( W_{xh}^{(\\ell)} z_t^{(\\ell)} + W_{hh}^{(\\ell)} h_{t-1}^{(\\ell)} + b_h^{(\\ell)} \\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W_{xh}^{(\\ell)} \\in \\mathbb{R}^{h_\\ell \\times d_\\ell}$ are the input weights for layer $\\ell$,\n",
    "- $W_{hh}^{(\\ell)} \\in \\mathbb{R}^{h_\\ell \\times h_\\ell}$ are the recurrent weights between timesteps layer $\\ell$,\n",
    "- $b_h^{(\\ell)} \\in \\mathbb{R}^{h_\\ell \\times 1}$ is the bias,\n",
    "- $z_t^{(\\ell)}$ is the input to this layer:\n",
    "  - For the first layer, $z_t^{(1)} = x_t$,\n",
    "  - For subsequent layers, $z_t^{(\\ell)} = h_t^{(\\ell-1)}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "The output at time step $t$ is computed from the last hidden layer $h_t^{(L)}$:\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} h_t^{(L)} + b_y,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W_{hy} \\in \\mathbb{R}^{d_\\text{out} \\times h_L}$ are the output weights,\n",
    "- $b_y \\in \\mathbb{R}^{d_\\text{out} \\times 1}$ is the output bias.\n",
    "\n",
    "---\n",
    "\n",
    "## Sequence Processing\n",
    "\n",
    "Given a sequence of inputs $\\{x_1, x_2, \\dots, x_T\\}$:\n",
    "\n",
    "1. Initialize all hidden states as zeros:\n",
    "   $$\n",
    "   h_0^{(\\ell)} = 0 \\quad \\text{for all } \\ell = 1, \\dots, L.\n",
    "   $$\n",
    "2. For each time step $t = 1, \\dots, T$:\n",
    "   - Update hidden states layer by layer using the recurrence above.\n",
    "   - Compute the output $y_t$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "We use the **Mean Squared Error (MSE)** loss between the predicted outputs \n",
    "$\\{y_t\\}$ and target outputs $\\{ \\hat{y}_t \\}$ across the sequence:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{T} \\sum_{t=1}^{T} \\| y_t - \\hat{y}_t \\|^2,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $T$ is the sequence length,\n",
    "- $y_t \\in \\mathbb{R}^{d_\\text{out}}$ is the network prediction at time $t$,\n",
    "- $\\hat{y}_t \\in \\mathbb{R}^{d_\\text{out}}$ is the target at time $t$.\n",
    "\n",
    "The gradient of the loss w.r.t. the output at time step $t$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_t} = \\frac{2}{T} \\big( y_t - \\hat{y}_t \\big).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "During training, gradients are propagated **backwards through time** \n",
    "and **down through the layers**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Output layer gradients\n",
    "\n",
    "For the output layer:\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} h_t^{(L)} + b_y,\n",
    "$$\n",
    "\n",
    "the gradients are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{hy}} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial y_t} \\, h_t^{(L)\\top},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_y} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial y_t},\n",
    "$$\n",
    "\n",
    "and the gradient w.r.t. the last hidden state is:\n",
    "\n",
    "$$\n",
    "\\delta_t^{(L)} = W_{hy}^\\top \\frac{\\partial \\mathcal{L}}{\\partial y_t}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Hidden layer gradients\n",
    "\n",
    "For each hidden layer $\\ell$ at time step $t$:\n",
    "\n",
    "$$\n",
    "h_t^{(\\ell)} = \\tanh\\!\\left( W_{xh}^{(\\ell)} z_t^{(\\ell)} + W_{hh}^{(\\ell)} h_{t-1}^{(\\ell)} + b_h^{(\\ell)} \\right),\n",
    "$$\n",
    "\n",
    "with $z_t^{(1)} = x_t$ and $z_t^{(\\ell)} = h_t^{(\\ell-1)}$ for deeper layers.\n",
    "\n",
    "The local derivative of the activation is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t^{(\\ell)}}{\\partial a_t^{(\\ell)}} = 1 - \\big(h_t^{(\\ell)}\\big)^2,\n",
    "$$\n",
    "\n",
    "where $a_t^{(\\ell)}$ is the pre-activation input to the $\\tanh$.\n",
    "\n",
    "The backpropagated error signal is:\n",
    "\n",
    "$$\n",
    "\\delta_t^{(\\ell)} = \\left( \\delta_t^{(\\ell)} + \\delta_{t+1}^{(\\ell)} \\cdot W_{hh}^{(\\ell)\\top} \\right) \\odot \\left(1 - \\big(h_t^{(\\ell)}\\big)^2\\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\odot$ is elementwise multiplication,\n",
    "- $\\delta_{t+1}^{(\\ell)}$ carries gradient from the next timestep,\n",
    "- $\\delta_t^{(\\ell)}$ carries gradient from higher layers at the same timestep.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Parameter gradients for hidden layers\n",
    "\n",
    "The gradients of parameters for each layer $\\ell$ are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{xh}^{(\\ell)}} = \\sum_{t=1}^T \\delta_t^{(\\ell)} \\, z_t^{(\\ell)\\top},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{hh}^{(\\ell)}} = \\sum_{t=1}^T \\delta_t^{(\\ell)} \\, h_{t-1}^{(\\ell)\\top},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_h^{(\\ell)}} = \\sum_{t=1}^T \\delta_t^{(\\ell)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Gradient clipping and updates\n",
    "\n",
    "To prevent exploding gradients, we apply **clipping**:\n",
    "\n",
    "$$\n",
    "g \\leftarrow \\max\\!\\left(-c, \\, \\min(g, c)\\right),\n",
    "$$\n",
    "\n",
    "for some threshold $c$.\n",
    "\n",
    "Finally, parameters are updated with **stochastic gradient descent (SGD)**:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, \\frac{\\partial \\mathcal{L}}{\\partial \\theta},\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Loss is computed as sequence-wide MSE.  \n",
    "- Gradients are propagated backward through both **time** (via recurrent connections) and **layers** (via depth).  \n",
    "- Parameter updates use SGD with gradient clipping.  \n",
    "\n",
    "This procedure allows the RNN to learn temporal dependencies in sequential data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea97315-b77f-42a2-a2d6-3896a0393831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiLayerRNN:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "\n",
    "        # Store weight matrices for each hidden layer\n",
    "        self.Wxh = []  # input in same timstep → hidden in same timstep\n",
    "        self.Whh = []  # hidden in prev timestep → hidden current timestep\n",
    "        self.bh = []   # biases\n",
    "\n",
    "        prev_size = input_size\n",
    "        for h_size in hidden_sizes:\n",
    "            self.Wxh.append(np.random.randn(h_size, prev_size) * 0.01)\n",
    "            self.Whh.append(np.random.randn(h_size, h_size) * 0.01)\n",
    "            self.bh.append(np.zeros((h_size, 1)))\n",
    "            prev_size = h_size\n",
    "\n",
    "        # Output layer\n",
    "        self.Why = np.random.randn(output_size, hidden_sizes[-1]) * 0.01\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "        # For convenience, put weights in lists of np arrays\n",
    "        self.Wxh = np.array(self.Wxh, dtype=object)\n",
    "        self.Whh = np.array(self.Whh, dtype=object)\n",
    "        self.bh = np.array(self.bh, dtype=object)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: list of column vectors (input_size × 1)\n",
    "        Returns: outputs, caches for BPTT\n",
    "        \"\"\"\n",
    "        hs = [np.zeros((h, 1)) for h in self.hidden_sizes]\n",
    "        outputs, cache = [], []\n",
    "\n",
    "        for x in inputs:\n",
    "            layer_input = x\n",
    "            new_hs = []\n",
    "            for l in range(self.num_layers):\n",
    "                h_prev = hs[l]\n",
    "                h = np.tanh(\n",
    "                    np.dot(self.Wxh[l], layer_input) +\n",
    "                    np.dot(self.Whh[l], h_prev) +\n",
    "                    self.bh[l]\n",
    "                )\n",
    "                new_hs.append(h)\n",
    "                layer_input = h\n",
    "            hs = new_hs\n",
    "            y = np.dot(self.Why, hs[-1]) + self.by\n",
    "            outputs.append(y)\n",
    "\n",
    "            cache.append((x, hs.copy()))  # store for BPTT\n",
    "\n",
    "        return outputs, cache\n",
    "\n",
    "    def compute_loss(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Mean squared error loss\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        for y, t in zip(outputs, targets):\n",
    "            loss += np.sum((y - t) ** 2)\n",
    "        return loss / len(outputs)\n",
    "\n",
    "    def backward(self, outputs, targets, cache, learning_rate=1e-2):\n",
    "        \"\"\"\n",
    "        Backpropagation Through Time (BPTT)\n",
    "        \"\"\"\n",
    "        # Gradients init\n",
    "        dWxh = [np.zeros_like(W) for W in self.Wxh]\n",
    "        dWhh = [np.zeros_like(W) for W in self.Whh]\n",
    "        dbh = [np.zeros_like(b) for b in self.bh]\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dby = np.zeros_like(self.by)\n",
    "\n",
    "        # Initialize hidden state gradients\n",
    "        dh_next = [np.zeros((h, 1)) for h in self.hidden_sizes]\n",
    "\n",
    "        # Backward through time\n",
    "        for t in reversed(range(len(outputs))):\n",
    "            y, target = outputs[t], targets[t]\n",
    "            dy = 2.0 * (y - target)  # dL/dy\n",
    "            dWhy += np.dot(dy, cache[t][1][-1].T)\n",
    "            dby += dy\n",
    "\n",
    "            dh = np.dot(self.Why.T, dy)  # gradient into last hidden layer\n",
    "\n",
    "            for l in reversed(range(self.num_layers)):\n",
    "                h = cache[t][1][l]\n",
    "                h_prev = np.zeros_like(h) if t == 0 else cache[t-1][1][l]\n",
    "\n",
    "                # Add gradient flowing from above in multi-layer case\n",
    "                dh += dh_next[l]\n",
    "\n",
    "                # Backprop through tanh: derivative (1 - h^2)\n",
    "                dtanh = (1 - h * h) * dh\n",
    "\n",
    "                # Gradients\n",
    "                dWxh[l] += np.dot(dtanh, cache[t][0].T if l == 0 else cache[t][1][l-1].T)\n",
    "                dWhh[l] += np.dot(dtanh, h_prev.T)\n",
    "                dbh[l] += dtanh\n",
    "\n",
    "                # Propagate gradient backwards in time and to lower layers\n",
    "                dh_next[l] = np.dot(self.Whh[l].T, dtanh)\n",
    "                dh = np.dot(self.Wxh[l].T, dtanh)  # pass to lower layer\n",
    "\n",
    "        # Clip gradients to prevent exploding\n",
    "        for dparam in dWxh + dWhh + dbh + [dWhy, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        # SGD update\n",
    "        for l in range(self.num_layers):\n",
    "            self.Wxh[l] -= learning_rate * dWxh[l]\n",
    "            self.Whh[l] -= learning_rate * dWhh[l]\n",
    "            self.bh[l]  -= learning_rate * dbh[l]\n",
    "        self.Why -= learning_rate * dWhy\n",
    "        self.by  -= learning_rate * dby\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2520a777-fd00-4cfc-b8a5-ab61ea5b77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6369\n",
      "Epoch 20, Loss: 1.5723\n",
      "Epoch 40, Loss: 2.2093\n",
      "Epoch 60, Loss: 2.2362\n",
      "Epoch 80, Loss: 2.1613\n",
      "Epoch 100, Loss: 1.8064\n",
      "Epoch 120, Loss: 2.6874\n",
      "Epoch 140, Loss: 2.4344\n",
      "Epoch 160, Loss: 2.1908\n",
      "Epoch 180, Loss: 2.0478\n",
      "Epoch 200, Loss: 1.8467\n",
      "Epoch 220, Loss: 1.4672\n",
      "Epoch 240, Loss: 1.8439\n",
      "Epoch 260, Loss: 2.0002\n",
      "Epoch 280, Loss: 1.2825\n",
      "Epoch 300, Loss: 1.4901\n",
      "Epoch 320, Loss: 2.2894\n",
      "Epoch 340, Loss: 2.2067\n",
      "Epoch 360, Loss: 2.1660\n",
      "Epoch 380, Loss: 1.7329\n",
      "Epoch 400, Loss: 1.7152\n",
      "Epoch 420, Loss: 1.8073\n",
      "Epoch 440, Loss: 1.6523\n",
      "Epoch 460, Loss: 0.8341\n",
      "Epoch 480, Loss: 1.4956\n",
      "Epoch 500, Loss: 1.7462\n",
      "Epoch 520, Loss: 2.8333\n",
      "Epoch 540, Loss: 2.3198\n",
      "Epoch 560, Loss: 2.3375\n",
      "Epoch 580, Loss: 2.2884\n",
      "Epoch 600, Loss: 1.9303\n",
      "Epoch 620, Loss: 1.3328\n",
      "Epoch 640, Loss: 2.4850\n",
      "Epoch 660, Loss: 2.1811\n",
      "Epoch 680, Loss: 2.6325\n",
      "Epoch 700, Loss: 2.4115\n",
      "Epoch 720, Loss: 1.5769\n",
      "Epoch 740, Loss: 1.5314\n",
      "Epoch 760, Loss: 2.5571\n",
      "Epoch 780, Loss: 2.3426\n",
      "Epoch 800, Loss: 1.9860\n",
      "Epoch 820, Loss: 2.8729\n",
      "Epoch 840, Loss: 1.6542\n",
      "Epoch 860, Loss: 1.6484\n",
      "Epoch 880, Loss: 2.6608\n",
      "Epoch 900, Loss: 3.2891\n",
      "Epoch 920, Loss: 1.9981\n",
      "Epoch 940, Loss: 0.8005\n",
      "Epoch 960, Loss: 1.5058\n",
      "Epoch 980, Loss: 2.0559\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    input_size = 2\n",
    "    hidden_sizes = [4, 6, 4]\n",
    "    output_size = 2\n",
    "    seq_length = 10\n",
    "\n",
    "    rnn = MultiLayerRNN(input_size, hidden_sizes, output_size)\n",
    "\n",
    "    # Dummy training data: try to learn identity mapping (predict next x = input)\n",
    "    for epoch in range(1000):\n",
    "        # Random input sequence\n",
    "        inputs = [np.random.randn(input_size, 1) for _ in range(seq_length)]\n",
    "        targets = inputs  # try to copy input\n",
    "\n",
    "        outputs, cache = rnn.forward(inputs)\n",
    "        loss = rnn.compute_loss(outputs, targets)\n",
    "        rnn.backward(outputs, targets, cache, learning_rate=1e-2)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c8a566-ea9e-4283-851a-239de564596d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.09457819,  0.0945782 ,  0.09457819,  0.09457819],\n",
       "         [-0.09716841, -0.09716842, -0.09716841, -0.09716842]]),\n",
       "  array([[ 0.09456884,  0.09456888,  0.09456883,  0.09456888],\n",
       "         [-0.09716798, -0.097168  , -0.09716797, -0.097168  ]]),\n",
       "  array([[ 0.09456858,  0.09456864,  0.09456855,  0.09456864],\n",
       "         [-0.09716798, -0.09716801, -0.09716797, -0.09716801]]),\n",
       "  array([[ 0.09456857,  0.09456866,  0.09456854,  0.09456867],\n",
       "         [-0.09716798, -0.09716802, -0.09716797, -0.09716803]]),\n",
       "  array([[ 0.09456858,  0.0945687 ,  0.09456853,  0.0945687 ],\n",
       "         [-0.09716799, -0.09716804, -0.09716797, -0.09716804]])],\n",
       " [(array([0. , 0.1]),\n",
       "   [array([[-0.00013748,  0.00151443, -0.00023568,  0.00075999],\n",
       "           [-0.0001385 ,  0.00151342, -0.0002367 ,  0.00075897],\n",
       "           [-0.00013656,  0.00151536, -0.00023475,  0.00076092],\n",
       "           [-0.00013802,  0.00151389, -0.00023622,  0.00075945]]),\n",
       "    array([[ 9.90486207e-06, -1.01750061e-05,  1.10984650e-05,\n",
       "            -1.00439567e-06],\n",
       "           [-1.41614348e-04, -1.35524051e-04, -1.41976372e-04,\n",
       "            -1.38305531e-04],\n",
       "           [ 6.36394750e-05,  3.92784124e-05,  6.50875639e-05,\n",
       "             5.04042731e-05],\n",
       "           [ 4.59167577e-05, -1.70370674e-06,  4.87474498e-05,\n",
       "             2.00448787e-05],\n",
       "           [ 8.97268652e-05,  9.80285437e-05,  8.92333905e-05,\n",
       "             9.42371115e-05],\n",
       "           [ 1.78091704e-04,  1.51435852e-04,  1.79676202e-04,\n",
       "             1.63609758e-04]]),\n",
       "    array([[-0.0031761 , -0.00317653, -0.00317607, -0.00317633],\n",
       "           [-0.0138098 , -0.01380907, -0.01380984, -0.01380941],\n",
       "           [ 0.00331662,  0.00331667,  0.00331662,  0.00331665],\n",
       "           [ 0.00334722,  0.0033472 ,  0.00334722,  0.00334721]])]),\n",
       "  (array([0.2, 0.3]),\n",
       "   [array([[ 0.00059885,  0.00584969, -0.00116857,  0.00545002],\n",
       "           [ 0.00060211,  0.00580171, -0.00116226,  0.00542544],\n",
       "           [ 0.00060276,  0.00581787, -0.00116254,  0.00543451],\n",
       "           [ 0.00059731,  0.00586036, -0.00117083,  0.00545511]]),\n",
       "    array([[ 3.96308735e-06, -5.97531564e-05,  2.54404072e-05,\n",
       "            -5.49452967e-05],\n",
       "           [-1.38806808e-04, -1.18886682e-04, -1.45356312e-04,\n",
       "            -1.20616551e-04],\n",
       "           [ 5.48786062e-05, -2.18278502e-05,  8.08997233e-05,\n",
       "            -1.62664831e-05],\n",
       "           [ 2.61338416e-05, -1.23817736e-04,  7.69997877e-05,\n",
       "            -1.12943069e-04],\n",
       "           [ 8.94711327e-05,  1.16206979e-04,  8.05683136e-05,\n",
       "             1.14039559e-04],\n",
       "           [ 1.69406123e-04,  8.53413525e-05,  1.97886288e-04,\n",
       "             9.14871733e-05]]),\n",
       "    array([[-0.00323844, -0.00323979, -0.00323798, -0.00323969],\n",
       "           [-0.01414739, -0.01414508, -0.01414817, -0.01414525],\n",
       "           [ 0.00354092,  0.00354106,  0.00354087,  0.00354106],\n",
       "           [ 0.00344706,  0.00344698,  0.00344709,  0.00344699]])]),\n",
       "  (array([0.4, 0.5]),\n",
       "   [array([[ 0.00132773,  0.01016044, -0.00209545,  0.0101061 ],\n",
       "           [ 0.00130801,  0.00997952, -0.00206045,  0.0099368 ],\n",
       "           [ 0.0013157 ,  0.01003501, -0.00206927,  0.00998924],\n",
       "           [ 0.00133162,  0.0102032 , -0.00210462,  0.01014587]]),\n",
       "    array([[-4.79136552e-06, -1.11817412e-04,  3.67017726e-05,\n",
       "            -1.11178690e-04],\n",
       "           [-1.35841123e-04, -1.01493437e-04, -1.49062267e-04,\n",
       "            -1.01829207e-04],\n",
       "           [ 4.44120453e-05, -8.35622291e-05,  9.41168422e-05,\n",
       "            -8.29222636e-05],\n",
       "           [ 5.78841481e-06, -2.44358304e-04,  1.02955933e-04,\n",
       "            -2.43123161e-04],\n",
       "           [ 9.32332057e-05,  1.38729810e-04,  7.56577354e-05,\n",
       "             1.38371513e-04],\n",
       "           [ 1.57868319e-04,  1.74348236e-05,  2.12396111e-04,\n",
       "             1.81591436e-05]]),\n",
       "    array([[-0.00324286, -0.00324512, -0.00324198, -0.00324511],\n",
       "           [-0.01415554, -0.01415165, -0.01415705, -0.01415168],\n",
       "           [ 0.00354818,  0.00354839,  0.00354809,  0.00354839],\n",
       "           [ 0.00345137,  0.00345124,  0.00345142,  0.00345124]])]),\n",
       "  (array([0.6, 0.7]),\n",
       "   [array([[ 0.00205658,  0.01447073, -0.00302232,  0.01476163],\n",
       "           [ 0.002015  ,  0.01416054, -0.00295951,  0.01445273],\n",
       "           [ 0.00202885,  0.01425252, -0.00297618,  0.01454455],\n",
       "           [ 0.0020659 ,  0.01454555, -0.00303838,  0.01483602]]),\n",
       "    array([[-1.36075937e-05, -1.63952373e-04,  4.79117484e-05,\n",
       "            -1.67489303e-04],\n",
       "           [-1.32906482e-04, -8.41399039e-05, -1.52793726e-04,\n",
       "            -8.30857288e-05],\n",
       "           [ 3.39687807e-05, -1.45311812e-04,  1.07393233e-04,\n",
       "            -1.49618244e-04],\n",
       "           [-1.46099726e-05, -3.64983521e-04,  1.28892639e-04,\n",
       "            -3.73410184e-04],\n",
       "           [ 9.70525145e-05,  1.61307549e-04,  7.08052788e-05,\n",
       "             1.62757131e-04],\n",
       "           [ 1.46391793e-04, -5.04316267e-05,  2.26988822e-04,\n",
       "            -5.51432535e-05]]),\n",
       "    array([[-0.00324319, -0.00324636, -0.00324189, -0.00324643],\n",
       "           [-0.01415543, -0.01414996, -0.01415767, -0.01414984],\n",
       "           [ 0.00354841,  0.00354869,  0.00354829,  0.0035487 ],\n",
       "           [ 0.00345144,  0.00345126,  0.00345152,  0.00345125]])]),\n",
       "  (array([0.8, 0.9]),\n",
       "   [array([[ 0.00278543,  0.01878051, -0.00394918,  0.01941654],\n",
       "           [ 0.00272197,  0.018341  , -0.00385855,  0.01896797],\n",
       "           [ 0.00274201,  0.01846954, -0.00388309,  0.01909928],\n",
       "           [ 0.00280017,  0.01888734, -0.00397213,  0.01952552]]),\n",
       "    array([[-2.24232845e-05, -2.16080950e-04,  5.91223896e-05,\n",
       "            -2.23792514e-04],\n",
       "           [-1.29972035e-04, -6.67891582e-05, -1.56525352e-04,\n",
       "            -6.43455196e-05],\n",
       "           [ 2.35278813e-05, -2.07050025e-04,  1.20670771e-04,\n",
       "            -2.16300599e-04],\n",
       "           [-3.50065319e-05, -4.85592455e-04,  1.54830448e-04,\n",
       "            -5.03677895e-04],\n",
       "           [ 1.00870168e-04,  1.83881719e-04,  6.59505593e-05,\n",
       "             1.87139136e-04],\n",
       "           [ 1.34916214e-04, -1.18288877e-04,  2.41582093e-04,\n",
       "            -1.28434745e-04]]),\n",
       "    array([[-0.00324338, -0.00324746, -0.00324166, -0.00324762],\n",
       "           [-0.01415512, -0.01414807, -0.01415809, -0.01414779],\n",
       "           [ 0.00354843,  0.00354878,  0.00354828,  0.00354879],\n",
       "           [ 0.00345144,  0.00345119,  0.00345154,  0.00345118]])])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.forward(np.array(list(range(seq_length))).reshape(-1,2)/seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356ce8d-b8ae-4621-9aa9-a637bf42dff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
